---
title: "ARCH249 Special Topic: R for Building Science"
subtitle: "Week 2 - Working with survey data in R"
author: "Tom Parkinson"
date: '`r format(Sys.time(), "%B,  %Y")`'
output:
  html_document:
    self_contained: yes
    df_print: paged
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    toc_depth: 2
    number_sections: yes
    theme: spacelab
---

```{r setup, include = FALSE}

# set global settings to hide console messages from markdown document
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = FALSE)

# load tidyverse package
library("tidyverse")

```


# Introduction

Surveys blah blah...hopefully you worked through the readings for the week and we all know what post-occupancy evaluations (POEs) are.

Let's start by making sure you've installed the `{tidyverse}` packages. To install packages, open RStudio and in the console (down the bottom) type `install.packages("tidyverse")`. Remember, the console is where you run code that only needs to happen once; a script (.R or .Rmd files) is when you are developing code that will run a series of commands.

The first part of any script is to load the necessary packages. Let's start the script by adding the following code.

``` {r}

# load tidyverse
library(tidyverse)

```

# Importing Data

In most cases, the next task for any data analysis workflow is to import data into your R environment. There are many different ways to encode store data. Arguably the most common is the comma-separated values file, or CSV. The humble csv uses 'delimited text' where values are separated by a comma. Each line of the file is a data record. You've almost certainly encountered a .csv file (perhaps in Excel), and we'll import one such file here to start our analysis. Note that R works with 'in-memory' data, meaning it loads and works with data directly from your computers memory (rather than a file on a disk drive).

## Base R

The ubiquity of .csv files means that R has an in-built function for importing them:

```{r eval = FALSE}

read.csv("my_csv_filename.csv")

```

## `readr::read_csv()`

The base R implementation will work, but (as with all things) the `tidyverse` implementation is better. The equivalent function comes from the `readr` package and is similarly named:

```{r eval = FALSE}

read_csv("my_csv_filename.csv")

```


Note the substitution of `.` with `_`. Many of the `tidyverse` functions exist in base R. There are often good reasons to use the `tidyverse` implementation, especially if you're using a it later in the analysis. We won't go into these now (they have to do with data types), but a good tip to spotting a `tidyverse` function: almost all functions in `tidyverse` use the underscore in place of the more conventional period.

## Arguments

In our earlier example, we did not explicitly set any of the arguments. While this works in most cases, there might be unexpected behaviour stemming from the use of defaults. We'll start with a simple one. Let's use `readr` to import our csv file.

``` {r}

# import csv file
survey_data <- read_csv("survey_data.csv")

```

We've just imported data into our working environment. We can inspect the data using the `head()` function.

``` {r}

# print head
head(survey_data)

```

The data looks as we'd expect it to. Each column is a variable - in this case a question in the survey - and each column is a response. Notice that `readr` was smart enough to assume that the column headers were in the first row of our .csv file. For arguments sake (pun intended), what would happen if we turn off that default? Let's set the `col_names` argument to `FALSE` when importing the .csv file and see what happens. 

```{r}

# read csv file without headers
survey_data <- read_csv("survey_data.csv", col_names = FALSE)

# print first five rows
head(survey_data, n = 5)

```

Something's up!? Notice how our headers are now `X1`, `X2` etc. And our actual header names are in the first row. That's because we told `readr` that our column names were not in the first row and it took us at our word. `tidyverse` tends to have seemingly logical defaults - more so than base R - but there are times when you'll want to interrogate the defaults to make sure things are working the way you expect. For reference, quickly scan all the possible arguments for `read_csv()` on the [help page](https://readr.tidyverse.org/reference/read_delim.html).

### **Exercise:** import data

Import the csv with the correct headers and print the first 10 rows of the data in the [Exercise 1](#exercise_1) code chunk. Make sure to give the object a name you'll remember - I recommend something explicit like `survey_data`.

``` {r exercise_1, echo = TRUE}

# read csv with headers


# print first 10 rows


```


## Data types

Last week we went over the theory of data types. You might remember words like integers or doubles or strings. These are how R understands the data you're working with at a low-level. It might not seem important but it really is. For example, `6` might look like a number to you but R could be storing it as a string. This becomes a problem if you wanted to add two "numbers" that are actually strings. Unfortunately, it's something you just have to learn over time. But we can point to a few clues to help you along the way. 

One key motivation for using `readr` over base R's csv import is that it makes a very good attempt at guessing the data types. There's even an argument to tell it how hard to look (look at the `guess_max` argument). Let's preview our data again and take a look at how well it guessed the data types. Take note of the text below the column names e.g. `<chr>`.

```{r}

# read csv file without headers
survey_data <- read_csv("survey_data.csv", col_names = TRUE)

# print first five rows
head(survey_data, n = 5)

```

**Question:** How do you think it went? What are some of the data types that you're seeing? And do they seem reasonable? Bonus points: does the `Recorded Date` column seem correct? 

We won't spend too much time interrogating these in our dataset right now, but we wanted to bring it to your attention for later.


# Data Wrangling

## Selecting data

We've imported our data into R and now we want to start to work with it. Let's quickly get an idea of what we are working with. I find myself using two arguments to do this: `dim()` and `summary()`.

```{r}

# check the 'dimensions' of the dataframe
dim(survey_data)

```

**Question:** Running `dim()` returned two numbers - what do you think they refer to? 

Now let's try `summary()`. Just a heads up that this will print quite a lot of text in your console; I've hidden the results for the sake of brevity.

```{r results = 'hide'}

# summarise the dataframe
summary(survey_data)

```

### **Exercise:** column names
A lot of the 'space' in the printout here are the headers (or column names), which in our dataset are the question strings. If we just wanted to know what those are we can use the `names()` function. Try that out in the [Exercise 2](#exercise_2) code chunk and see what it returns.

``` {r exercise_2}

# print just the column names


```

### `dplyr::select()`

I find it hard to work with a dataframe that contains 100+ columns. Many others disagree, but I prefer to make a new dataframe containing only the columns that I need so I can visually inspect the data while preserving the original dataframe. To do that I use the `select()` function. The `select()` function is used to choose which *columns* you want to keep in your dataframe. More details about `select()` can be found at the [help page](https://dplyr.tidyverse.org/reference/select.html). Note: if you want to select which *rows* to keep then you would use another function - we'll get to that in a bit.

In our [analysis of the CBE Occupant Survey database](https://journal-buildingscities.org/articles/10.5334/bc.76/), we found that workspace acoustics are the biggest problem area in offices. We'll analyse the acoustics questions to see how this particular office rates. Let's start by selecting the questions that relate to acoustics. We'll create a new dataframe and call it `acoustics_data`. Our column names are a bit clunky as they are full strings, so `names()` might be useful here to jog your memory. 

The arguments for `select()` are very straightforward - the names of the columns you want to keep separated by commas. We have spaces in our column names so we need to wrap them in quotation marks e.g. `select(Response_ID)` becomes `select("Response ID")` when there is a space. Quotation marks also work for stings without spaces, too.

```{r}

# select the columns we need to analyse acoustics-related questions
acoustics_data <- survey_data %>%
  select("Response ID",
         "Progress",
         "How satisfied are you with the... - Noise level in your workspace",
         "How satisfied are you with the... - Sound privacy in your workspace (ability to have communications without your neighbors overhearing and vice versa)",
         "Overall, does the acoustic quality in your workspace enhance or interfere with your ability to get your job done?")

```

**Question:** Wait...what's this weird `%>%` thing we've just used?

There's a lot of text in the above code chunk, so let's break down what just happened. First, we assign the output of our code chunk to an object named `acoustics_data`. Then, we take our `survey_data` object and we "pipe" it (i.e. pass it to) the `select()` function. Within the `select()` function, we nominate the names of the five columns that we'd like to keep in our new dataframe.

### **Exercise:** create new dataframe
Include one more acoustics-related question in our new `acoustics_data` dataframe in the [Exercise 3](#exercise_3) code chunk. Tip: look for a question about *dissatisfaction* with the acoustic environment. Then inspect the dataframe using the `head()` function.

```{r exercise_3}

# repeat select() but with an additional acoustics question

# print first n rows of new acoustics dataframe

```


## Renaming columns

While having the full question string as a column name makes sense initially, it becomes very clumsy working with such long names in code. There's also the added complications of having spaces in the column names, which should be avoided where possible. So to calm my nerves, we are going to quickly rename the columns so it's easier to work with in the subsequent code. 

**Question:** Any guesses as to what the function to rename is called?

Let's rename the `Response ID` and the `Progress` columns by doing three things:

1. Shorten names where possible

2. Removing spaces

3. Using lowercase

``` {r}

# rename the response id column
acoustics_data <- acoustics_data %>%
  rename("response_id" = "Response ID",
         "progress" = "Progress")

```

Note that the argument for `rename()` uses the syntax `new_name = old_name`. Inspect your dataframe and see what happened to the name of the first two columns. This is going to be much easier to work with moving forward because you won't have to wrap the column names in quotation marks when referring to them. 

### **Exercise:** rename columns
Rename the other columns using the above rules in the [Exercise 4](#exercise_4) code chunk. Make sure they are easy to understand as you'll have to remember them later on. For example, *"How satisfied are you with the... - Noise level in your workspace"* could become `noise_level`. And *"Overall, does the acoustic quality in your workspace enhance or interfere with your ability to get your job done?"* could become `overall_acoustics`.

``` {r exercise_4}

# rename the other columns
acoustics_data <- acoustics_data %>%
  rename("noise_level" = 3,
         "sound_privacy" = 4,
         "overall_acoustics" = 5)

```


## Filtering data

We just learned how to select and rename columns from our dataframe. The `select()` function is column-wise, but what about selecting particular rows? This is known as filtering, and the `{dplyr}` package has a handy function for that too!

### `dplyr::filter()`

Filtering is when you want to subset your data row-wise. This is useful if you only want to keep rows where a particular condition is met. For our dataset, each row is a survey respondent. One of the columns that we kept in the earlier section was the progress that each made respondent made through the survey. So a value of 100 means they completed the survey, while a value of 0 means they did not answer any questions. That seems like a good value to filter on! As always, more details about `filter()` can be found at the [help page](https://dplyr.tidyverse.org/reference/filter.html).

In much the same way as we did when using `select()` to choose the columns, we can use `filter()` to choose particular rows. The arguments for `filter()` are the conditions upon which we base the filtering. Using our progress column, we are going to filter responses that completed at least half of the survey. You can quickly check the number of rows *before* filtering using the `nrow()` function.

```{r}

# count the number of rows before filtering
nrow(acoustics_data)

# filter data based on survey progress
acoustics_data <- acoustics_data %>%
  filter(progress >= 50)

# count the number of rows after filtering
nrow(acoustics_data)

```

We went from a dataframe of 135 rows to a dataframe of 117 rows. This tells us that `r 135-117` respondents completed less than half of the survey. We've just removed those responses from our dataset. 

**Question:** Inspect your new dataframe now that we've dropped some respondents. Does it look like a 'complete' dataset to you?

It seems that our filtering missed a few cases, as shown by the `NA` responses. This tells us that while those respondents *did* complete more than half of the survey they *did not* answer the acoustics questions that we are analyzing. Luckily for us, `filter()` accepts multiple criteria. Let's keep our first condition around survey progress but add another condition that checks for `NA` values also. We're going to introduce two new concepts here:

1. The **NOT** check with `!` i.e. this is *not* true

2. Checking if a value is missing using the `is.na()` function

```{r}

# filter the acoustics dataset based on two conditions
acoustics_data <- acoustics_data %>%
  filter(progress >= 50 & !is.na(noise_level))

# count the number of rows after filtering
nrow(acoustics_data)

```

**Question:** what does `!` combined with `is.na()` do to our conditional logic?

Much like the last filter, we reduced the length of the dataframe from 117 rows to 112. From this we can infer that 5 rows had `NA` values in the `noise_level` column. I think the dataframe is reasonably 'clean' and we can start to better understand the occupant responses.

**Side Quest:** the `{tidyr}` package has the `drop_na()` function. It is often easier to use that than to add a condition when filtering. See if you can combine `filter()` and `drop_na()` using pipes to reach the same result as we did in the above code chunk.

# Factors

So far we've been working with `integer` (progress column) and `character` (responses) data types. We haven't had too many issues, but now is the perfect time to introduce the `factor` data type. Factors are used to categorize data i.e. categorical data. An example of categorical data is gender, where there are a limited number of unique categories. Factors can be unordered (e.g. gender) or ordered (e.g. "Very Dissatisfied" is worse than "Satisfied"). Factors are stored as 'levels' and can be applied to both integers and characters in R. All of these attributes make factors an incredibly useful data type when working with survey data.

Factors are one of the most common data types, and are natively supported in R. The `tidyverse` package for working with factors is `{forcats}`, and we'll work through some basic functions that are useful for our analysis.

## Converting to factor
The most simple (and useful) task is converting a column of values to a factor. In our dataset, we'd like to convert the responses to our acoustic questions from characters to factors. As with most things, there's the base R approach and the `tidyverse` approach. Let's try both as an example of how there are multiple ways to do the same thing in R. First, the base R approach. Reminder: the `$` operator is used to nominate a column from a data frame e.g. `object_name$column_name`.

``` {r}

# convert acoustics questions to factors
acoustics_data$noise_level <- as.factor(acoustics_data$noise_level)
acoustics_data$sound_privacy <- as.factor(acoustics_data$sound_privacy)
acoustics_data$overall_acoustics <- as.factor(acoustics_data$overall_acoustics)

```

That was reasonably easy, right? We tell R that we want to take a column and convert it to a factor using `as.factor()`. The argument is simply the name of the column. Use `str()` to double check that it worked as we want it to.

``` {r echo = FALSE}

# check structure of dataframe
str(acoustics_data)

```

The print out of `str()` shows us that those columns that were characters are now factors. And there are 7 levels within each of those factors. That makes sense given responses to our survey questions were given on 7-point Likert scales. We can use base R to show us all the levels within a factor, too.

``` {r}

# print out all levels within one of our factor columns
levels(acoustics_data$noise_level)

```

## `tidyverse`
We've already told R that the responses are actually factors rather than characters. We can achieve the same thing in `tidyverse` with the added bonus of wrapping it up in one command. We're going to use our helpful new friend `mutate()` to modify the existing columns to convert them from a character to a factor.

``` {r}

# convert response columns to factors
acoustics_data <- acoustics_data %>%
  mutate(noise_level = as.factor(noise_level),
         sound_privacy = as.factor(sound_privacy),
         overall_acoustics = as.factor(overall_acoustics))

```

Just to reiterate what happened - we took the `acoustics_data` object, mutated (i.e. modified) the three columns, and converted them to factors. Check the data types using `str()`. Same result, different approach!

**Side Quest:** if you want to modify a number of columns in exactly the same way, then check out the `across()` function combined with `mutate()`. Perfect for those obsessed with elegant, minimal code! Full details are [here](https://dplyr.tidyverse.org/reference/across.html).

## Factor levels
Levels are the unique values within a factor. For example, the question "What is your marital status?" might have 3 levels of "Single", "Married" and "Divorced". Factors and levels are the ideal way to store responses to questions with a limited number of predefined options e.g. radio box questions. We've converted our questions to factors, each with 7 levels. Use `levels()` to remind yourself of what the factors are.

We've successfully made the responses factors but we haven't yet solved the problem of the level order. The output of `levels()` is showing that the ordering of the responses is alphabetical, when we know there is a better way to order them. The `factor()` function has the `levels` argument where we can explicitly define the order of the levels. Here's an example.

``` {r}

# define the level order for 'noise_level'
acoustics_data <- acoustics_data %>%
  mutate(noise_level = factor(noise_level, 
                              levels = c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied",
                                         "Neither satisfied nor dissatisfied", 
                                         "Somewhat satisfied", "Satisfied", "Very satisfied")))

```

**Question:** what is the `c` doing within our `levels` argument?

Check the levels of `noise_level` again and see what happened to the order. Success! R know knows that there is an order to the levels within our factor. This doesn't seem very important now but it will make your life so much easier when you come to visualize your results. It's also important if you come to do statistical tests, although you'd want to explore the `is.ordered()` argument if you find yourself there. For us, simply changing the order in which they are printed is sufficient.

### **Exercise:** factor reordering
Change the order of the other columns in the [Exercise 5](#exercise_5) code chunk to better represent the underlying data. You can use the above code chunk as an example. Make sure to check the levels as the questions might use different scales.

``` {r exercise_5}  

# define the level order for the other questions

```

## Summarising data

One of the first tasks you'll want to do with a new dataset is to provide simple summaries like counts (frequencies), averages, etc. These are almost always reported somewhere in your work, and are also a good opportunity for you (the analyst) to get early insights on the dataset. The `{tidyverse}` packages have a number of really useful functions to help summarise data. 

### `dplyr::count()`

The data we are working with is non-numeric, so we wouldn't use things like means and standard deviations. We're going to show you these things next week. For now, we'll start our analysis by simply counting the number of responses for each of the categories. Again, this is simple to do with `tidyverse` combined with the all-mighty `%>%` (pipe)! Let's count the number of different responses to our `noise_level` question.

``` {r}

# count responses to acoustics question
noise_summary <- acoustics_data %>%
  count(noise_level)

```

What happened here is that `{dplyr}` found all the unique values in the `noise_level` column and counted their occurrences. Simple! The arguments for `count()` are the columns that you wish to count the unique occurrences.

### **Exercise:** counts

Summarise the other columns using `count()` in the [Exercise 6](#exercise_6) code chunk. Try looking at the `dplyr::count()` [help page](https://dplyr.tidyverse.org/reference/count.html) to work out the correct syntax for the arguments.

``` {r exercise_6}

# count the responses for the other questions

```

### `dplyr::mutate()`

The table showing the counts is nice, but what if we wanted to include the percentages as well as the counts? That would be a useful thing to do. In `tidyverse` parlance, adding a new column (or editing an existing one) is called mutating and is done with the `mutate()` function. You should use `mutate()` when you want to:

1. Add a new column to your dataframe

2. Modify (overwrite) an existing column in your dataframe

Including the percentages in our table is an example of adding a column, and `mutate()` is going to help us with that. To do that, we will use `mutate()` and a little bit of arithmetic as follows.

``` {r}

# add percentages to summary table
noise_summary <- noise_summary %>%
  mutate(percs = n / sum(n))

```

Take a look at the table and see if that worked. Great...we now have percentages as well as the counts for the different responses to our question about noise. The `mutate()` function is one you'll be using over and over, so I recommend reading the [help page](https://dplyr.tidyverse.org/reference/mutate.html) if you need a bit more context.

But there's something bugging me about this table...

**Question:** why is our summary table ordered the way it is?

You've heard us keep blabbing on about data types and how important they are. Well, here's a practical example of that. Underneath the column header in the above table you'll see it says `<chr>`. That is the clue you'd need to answer the question of why the table is ordered the way it is.


# Power of the `%>%`

We just worked through a series of steps where we imported the survey responses, wrangled the data, and made sure the data types were correct for any subsequent analysis. Go us! We did this through a series of commands that broke down the workflow into discrete steps. But...do we need to do this with separate commands? Now is a good time to demonstrate the power of the pipe! Our famous workhorse `%>%` can combine all these steps into one so that we spend less time cleaning and more time doing other more exciting stuff. 

### **Exercise:** data wrangling
String together the data importing and wrangling tasks together into one command in the [Exercise 7](#exercise_7) code chunk. You can copy-paste some of the above code into one command connected with `%>%` that does all the tedious work.

``` {r exercise_7}  

# use %>% to connect the data import and wrangling tasks into one command

```

# Data Visualization

This is why we are all here...to make sexy plots 👀 I don't need to explain what data visualizations are, but if you're needing a reminder of **how** visualizations can look beautiful as well as **why** it's important to do so then I recommend [Information is Beautiful](https://informationisbeautiful.net).

One of the biggest strengths of R over other programming languages is the ability (and ease) to make very beautiful visualizations. And the best package for visualizations unsurprisingly comes from the `{tidyverse}`. It's known as `{ggplot2}` and is my favourite part of this entire enterprise. `{ggplot2}` is the workhorse for plotting tidy data, allowing you to make basically any kind of vizualisation you like. We're going to spend the remainder of today's class working in `{ggplot2}` to make a simple visualization of our survey data.

## Grammar of Graphics
The way that `{ggplot2}` works is based on a philosophy known as the "Grammar of Graphics". The book was originally published in 1999, well before `{ggplot2}` existed. But the book had such a big impact that it continues to inspire the way we visualize data today. I recommend reading it if the theory of data visualization interests you. For everyone else (myself included), there's the more practical [ggplot2 book](https://ggplot2-book.org/introduction.html) that has extensive documentation on using the `{ggplot2}` package.

I don't want to spend too much time on the background piece, but what's most essential to understanding how "Grammar of Graphics" inspired `{ggplot2}` is the idea of *aesthetic layers*. Put simply, any visualization can be broken down into a number of discrete layers, each containing important aesthetic information relevant to interpreting the data. There are three grammatical elements that are important for today:

1. Data: the data that you wish to plot

2. Aesthetics: the scales onto which we *map* our data e.g. the x- and y-axis

3. Geometries: the shape visual elements used in our plot e.g. bars, lines etc.

You'll see reference to these elements throughout the `{ggplot2}` code. For example, `aes()` is the function to define the mapping of data. And `geom_` is used to specify the type fo plot you want e.g. `geom_bar()`.

## `{ggplot2}`

There are so many different types of data visualizations (I'll refer to them as plots) that you can make in `{ggplot2}`. I think the [R Graph Gallery](https://r-graph-gallery.com/ggplot2-package.html) provides a useful overview of many of the options as well as example code.

**Question:** which kind of plot will work best with our data?

Let's get into it! We start any plot with the `ggplot()` function. The two arguments of the `ggplot()` function are `data` and `mapping`. `data` is straightforward - it's just our dataframe that we've been working on. `mapping` is a bit more abstract, but basically contains the variables to include on the x- and y-axis. We're going to start with a stacked column graph of only one question, so we don't need to define our x-axis. We have our data ready, so our code to start our plot will look something like:

``` {r}

# start plot of noise level
ggplot(data = acoustics_data, mapping = aes(y = noise_level))

```

Okay, something happened. We made something, but it doesn't really look like a data visualization. Actually, what we got is exactly what we expected. All we did in the earlier code is to ask for a plot (with the `ggplot()` function), point to the underlying data (with the `data = ` argument), and map some aesthetics (with the `mapping = aes()` argument). So far, so good.

**Question:** what are we missing?

## Adding layers

Remember how we talked about plots being made up of a number of layers. We just made the base layer; that is, the plot itself! But we didn't define any additional layers. Probably the next most important layer will be the **geometry**. The geometry defines the *type* of plot that we'd like to make. For this example we are going to make a bar chart, so we're going to include the `geom_bar()` function to our `ggplot()` code.

Before we do that, it's worth working through the syntax of layers in `{ggplot2}`. We've been using the pipe `%>%` throughout the data wrangling exercises as a way to join commands together. It would seem logical that the same syntax is used to add layers of a plot together. **However**, in `{ggplot}` we use a different syntax which is easy to understand but also easy to forget. To combine layers we simply use `+`. Think of the concept of *adding* layers together to make a plot. You'll likely confuse this with `%>%` as you're learning how to use all these new tools - that's totally okay!

Coming back to our bar chart, let's add our geometry to the plot.

```{r}

# start plot of noise level and add geometry
ggplot(data = acoustics_data, mapping = aes(y = noise_level)) +
  geom_bar()

```

We did it...we made our first plot 😍 It's not the most beautiful thing in the world, but it visually represents the underlying data! As such, we could stop there and be done with it. But we want to make it a bit more appealing, so let's walk through *some* of the options available to us in `{ggplot2}`.

## Labels

One of the simplest things you can do is change the labels. This includes the title and axis labels. You can do these individually, but there's a helpful `{ggplot2}` function `labs()` which combines them all into one. The arguments are the element you're wanting to change, and the values are the strings to use. Let's change our x- and y-axis labels, and add a title to our plot. Remember, we add layers using `+`.

```{r}

# start plot, add geometry, and change labels
ggplot(data = acoustics_data, mapping = aes(y = noise_level)) +
  geom_bar() +
  labs(title = "My First Plot", subtitle = "Satisfaction with the noise level in the office",
       x = "Response Count", y = NULL)

```

**Question:** what do you think `NULL` is doing here?

## Themes

`{ggplot2}` does a good job of making a plot without much work. But there are many cases where we'd want to change the look of the plot. Thankfully, there are inbuilt themes that we can use to quickly change the look of a plot. There's a helpful [list of themes](https://ggplot2.tidyverse.org/reference/ggtheme.html) on the `{ggplot2}` website.

### **Exercise:** plot theme
Apply `theme_classic()` to your plot in the [Exercise 8](#exercise_8) code chunk.

``` {r exercise_8}  

# define the theme of your plot

```

Nice one! That's one step closer to the plot of our dreams. It's often helpful to use the default themes to get *close* to the look you're after, and then modify individual elements to suit. Tweaking plots can be an exhaustive and time-consuming process, so we won't spend much time on it today. But to do so, you use the `theme()` function, with each argument defining some aspect of the plot. As an example, we'll turn remove the y-axis line, disable tick marks, and add break lines on the x-axis.

```{r}

# remove y-axis line and ticks
ggplot(data = acoustics_data, mapping = aes(y = noise_level)) +
  geom_bar() +
  labs(title = "My First Plot", subtitle = "Satisfaction with the noise level in the office",
       x = "Response Count", y = NULL) +
  theme_classic() +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        panel.grid.major.x = element_line())

```

## Colours

One thing that always changes with a plot are the colours! It's not uncommon for me to spend 30-minutes just getting the colours right for a plot. The more complex and detailed the plot, the harder it is to define a colour palette. Colours are defined in `{ggplot2}` using hex codes, which are a simple alphanumeric string of 6 characters with a hash prefix e.g. `#f568d2`. Obviously I never know what the hex code of the colour I want is, so I use [coolors](https://coolors.co) to help find colours and copy their hex code into my plot. To change the colour of our bars, we need to define it in the `fill` argument of `geom_bar()`.

```{r}

# remove y-axis line and ticks
ggplot(data = acoustics_data, mapping = aes(y = noise_level)) +
  geom_bar(fill = "#D2AFE9") +
  labs(title = "My First Plot", subtitle = "Satisfaction with the noise level in the office",
       x = "Response Count", y = NULL) +
  theme_classic() +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        panel.grid.major.x = element_line())

```

## Geometry

We talked a bit about the geometries of a plot; that is, the shape. In the earlier example we were working with `geom_bar()`, which is a type of a bar chart. We got the plot to work to a point where we could reasonably expect someone to be able to interpret the underlying data. But there are probably better ways for us to visualize this data. There's a helpful overview of the geometries [here](https://ggplot2.tidyverse.org/reference/). Let's quickly explore a tweak to our geometry which might communicate our message more clearly. Specifically, I'm thinking of a *stacked* bar chart. 

Remember we made a simple summary table of our noise level question that included the percentage of votes? You probably named the object `noise_summary`. Let's use that to make our stacked bar chart which shows the percentages rather than the absolute number of votes. We named our column `percs` which we'll map to our y-axis.

``` {r}

# make a stacked bar chart
ggplot(noise_summary, aes(x = 1, y = percs, fill = noise_level)) +
  geom_bar(position = position_fill(), stat = "identity") +
  coord_flip()

```

Note three important things here:

1. We didn't write out the argument names. We don't have to in most cases as R is smart enough to work out which is which if they are in the right order.

2. We used the `position` and `stat` arguments in `geom_bar()` to tell it to stack the votes (position) and leave the data as is rather than using the default count (stat).

3. We added `coord_flip()` so the bar runs horizontal rather than vertical.

Finally, let's prettify it a bit. I won't explain what is going on here - it's more for your reference to follow up on in your own time.

``` {r}

# prettify our stacked bar chart
ggplot(noise_summary, aes(x = 1, y = percs, fill = noise_level)) +
  geom_bar(position = position_fill(reverse = TRUE), stat = "identity", width = 0.05) +
  geom_text(aes(label = scales::percent(percs, accuracy = 1L)), 
            position = position_stack(vjust = 0.5, reverse = TRUE),
            colour = "grey20") +
  scale_x_continuous(limits = c(0.9, 1.10)) +
  scale_y_continuous(expand = c(0, 0), breaks = c(0.25, 0.5, 0.75), labels = c("25%", "50%", "75%")) +
  scale_fill_manual(values = c("#ba3b50", "#cf6b61", "#d09da9", "grey85", "#c2e8bb", "#8bcc8a", "#4cb15e")) +
  labs(title = "Office Noise Level", 
       subtitle = "Breakdown of 112 responses to questions about satisfaction with noise level",
       x = NULL, y = NULL, fill = NULL) +
  guides(fill = guide_legend(label.position = "bottom", nrow = 1)) +
  coord_flip() +
  theme_minimal(base_size = 10) +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 11, face = "italic", hjust = 0.5),
        axis.text = element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom",
        legend.direction = "horizontal")

```

## Saving plots

Once you've got your sexy plot, you're probably going to want to save it to your drive for later use. This is easy to do with ggplot using the `ggsave()` function. This is preferable over exporting from the plot preview as you have control over the dimensions and resolutions of the plot. The arguments in `ggsave()` are the width and height (defaults to inches) which you can use to properly proportion your plot.

``` {r}

# save plot to working directory
ggsave("my_plot_name.png", width = 7, height = 4, dpi = 300)

```


## **Exercise:** another geom

We've used `geom_bar()` so far to plot our data. Let's try using `geom_point()` in the [Exercise 9](#exercise_9) code chunk to see if there is a relationship between noise level and one other question. Tip: define one question as x and the other as y in the aesthetic mapping.

``` {r exercise_9}  

# define the theme of your plot

```

# `{tidyverse}` AMA

Okay I think we've covered most of the basics today. Now I'm going to hand it over to you to play around with the data, try to do things, inevitably break it, and then ask us what went wrong 🤠
